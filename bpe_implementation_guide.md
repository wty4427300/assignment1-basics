# CS336 赋值 1：BPE 分词器实现指南

本指南为完成作业要求的字节对编码（BPE）分词器提供了详细的、一步一步的演练。

---

### 第一部分：需要理解的核心概念

在开始编码之前，请确保你理解了这些关键概念。

#### 1. Unicode vs. UTF-8 (PDF Page 4-5)
- **为何重要**: 计算机处理的是字节（0-255的整数），而不是字符。Unicode 是字符集（例如 '牛'），而 UTF-8 是将字符转换为字节序列的编码方案。
- **作业要求**: 这是一个**字节级**的 BPE。我们合并的是字节（例如 `b't'`、`b'h'`），而不是字符。这使得分词器能够处理任何语言的任何文本，而不会出现“未知词元”的错误。

#### 2. BPE 算法 (PDF Page 6)
- **它是什么**: 一种数据压缩算法，通过迭代地合并最频繁出现的相邻词元对来工作。
- **流程**:
  1. 从一个基础词汇表开始（对我们来说，是所有256个字节）。
  2. 统计语料库中所有相邻字节对的频率。
  3. 找到最频繁的对（例如 `('t', 'h')`）。
  4. 将这对组合并成一个新的、更长的词元 (`'th'`)。
  5. 将新词元添加到词汇表中。
  6. 在整个语料库中，用新的 `'th'` 词元替换所有 `('t', 'h')` 对。
  7. 重复此过程，直到达到目标词汇表大小。

#### 3. 预分词 (Pre-tokenization) (PDF Page 6)
- **为何需要**: 在一个巨大的单个文本文件中统计字节对的频率，计算成本极高且效率低下。
- **方法**: 首先，使用一个正则表达式（PDF中指定的GPT-2正则表达式）将文本分割成一个“单词”或“词块”的列表。然后，BPE的合并操作**只在这些词块内部进行**，绝不跨越它们的边界。
- **关键点**: 你必须使用PDF中提供的那个复杂的正则表达式。

#### 4. 特殊词元 (Special Tokens) (PDF Page 7)
- **它们是什么**: 像 `<|endoftext|>` 这样的元数据词元，它们有特殊的含义，不应该被BPE算法分解。
- **如何处理**: 将它们视为不可分割的原子单位。它们应该在预分词之前被处理，并作为固定成员添加到词汇表中。

---

### 第二部分：实现任务与交付成果

这是你需要编码的内容。它被分解为PDF中的两个主要问题。

#### 任务 1: 训练 BPE 分词器 (`train_bpe`) (PDF Page 9)

这是作业中最具挑战性的部分。你需要编写一个**独立的函数**（不是 `Tokenizer` 类的方法），它接收一个文本文件并训练一个BPE模型。

- **建议的函数签名**:
  ```python
  def train_bpe(input_path: str, vocab_size: int, special_tokens: list[str]) -> (dict[int, bytes], list[tuple[bytes, bytes]]):
  ```
- **输入**:
  - `input_path`: 训练数据的路径 (例如 `tinystories_sample.txt`)。
  - `vocab_size`: 最终词汇表的目标大小。
  - `special_tokens`: 特殊词元字符串的列表。
- **输出**:
  - `vocab`: 一个从整数ID到字节的字典。
  - `merges`: 一个 `(byte, byte)` 元组的有序列表，代表了合并操作。

- **详细实现步骤**:
  1. **初始化词汇表**: 创建一个包含所有256个字节（ID 0-255）的基础词汇表。然后，加入你的特殊词元。
  2. **读取并分块语料库**:
     - 读取 `input_path` 的全部内容。
     - **处理特殊词元**: 使用 `re.split` 和特殊词元模式来分块语料库。这能确保特殊词元作为边界。
  3. **预分词和计数**:
     - 对每个块，使用第6页的GPT-2正则表达式进行预分词，得到一个“单词”列表。
     - 统计每个唯一单词的频率，得到一个像 `{'the': 500, 'a': 450, ...}` 这样的字典。
  4. **准备合并**: 将每个单词（字符串）转换为其字节成分的列表 (例如 `'dog'` -> `[b'd', b'o', b'g']`)。
  5. **主合并循环**: 这是核心算法。你需要循环 `vocab_size - len(initial_vocab)` 次。
     - **a. 统计词对频率**: 遍历你所有的单词（及其频率），统计所有相邻字节对的频率。
     - **b. 找到最高频的词对**: 找到频率最高的词对。PDF（第7页）指明了按字典序来打破平局。
     - **c. 执行合并**: 创建一个新的合并后的词元 (例如 `b'th'`)。
     - **d. 更新状态**:
       - 将新的合并规则 `(b't', b'h')` 添加到你的 `merges` 列表中。
       - 将新词元 `b'th'` 添加到你的 `vocab` 中。
       - 在你所有的单词表示中，将序列 `[b't', b'h']` 替换为 `[b'th']`。
     - **e. 重复**: 继续下一次迭代。
  6. **返回结果**: 返回最终的 `vocab` 和 `merges`。

#### 任务 2: 实现 `Tokenizer` 类 (PDF Page 11)

一旦你有了训练好的 `vocab` 和 `merges`，你需要完成 `cs336_basics/Tokenizer.py` 中的 `Tokenizer` 类，使其能够工作。

- **`__init__`**: 大部分已为你提供。它接收 `vocab`、`merges` 和 `special_tokens` 来设置分词器的内部状态。
- **`from_files(...)`**: **(你实现)** 一个类方法，用于从文件加载 `vocab` 和 `merges`，以方便地创建一个 `Tokenizer` 实例。
- **`encode(text: str)`**: **(你实现)** 编码函数。
  1. 首先，按特殊词元分割文本。
  2. 对每个子文本，使用GPT-2正则表达式进行预分词。
  3. 对每个产生的单词，将其转换为字节序列。
  4. **按顺序**应用学到的 `merges` 规则，将字节序列合并成更大的块。
  5. 使用 `inverse_vocab`（一个预先计算好的 `bytes -> int` 映射）将最终的字节块转换为Token ID。
  6. 连接所有的ID并返回列表。
- **`decode(ids: list[int])`**: **(你实现)** 解码函数。这个相对直接。
  1. 遍历ID列表，使用 `vocab` 查找每个ID对应的字节。
  2. 将所有字节连接成一个单独的 `bytes` 对象。
  3. 使用 `.decode("utf-8", errors="replace")` 将最终的 `bytes` 对象解码为字符串。

---

### 第三部分：需要阅读和使用的文件

- **`cs336_spring2025_assignment1_basics.pdf`**: **你的“圣经”**。所有的要求和提示都在这里，特别是 **Section 2 (Pages 4-13)**。
- **`cs336_basics/Tokenizer.py`**: 你将在这里实现 `Tokenizer` 类。我已经帮你把它改成了使用 `regex` 包。
- **`cs336_basics/pretokenization_example.py`**: **强烈推荐**。正如PDF第8页所指出的，预分词是一个性能瓶颈。这个文件包含了并行化处理的辅助代码，将极大地加速你的 `train_bpe` 函数。
- **`tests/test_train_bpe.py`** & **`tests/test_tokenizer.py`**: **你必须通过这些测试**。阅读这些文件是了解你的函数和方法确切的预期输入和输出的最佳方式。
- **`tests/adapters.py`**: 你需要在这个文件中编写一些“胶水代码”，将你的实现（如 `train_bpe` 函数）连接到测试框架。测试文件会调用这里的适配器函数。
